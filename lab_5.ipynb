{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Learning Backware Propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/babi/miniconda3/envs/tenx_week3/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rurwL-W2PoIN"
      },
      "source": [
        "## Creating Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "owY9mN1oPAal"
      },
      "outputs": [],
      "source": [
        "class DenseLayer:\n",
        "    \"\"\"\n",
        "    A simple dense layer for neural networks.\n",
        "\n",
        "    Parameters:\n",
        "    - n_inputs (int): Number of input features.\n",
        "    - n_neurons (int): Number of neurons in the layer.\n",
        "\n",
        "    Attributes:\n",
        "    - weights (torch.Tensor): The weights matrix for the layer.\n",
        "    - biases (torch.Tensor): The biases vector for the layer.\n",
        "    - output (torch.Tensor): The output values after a forward pass.\n",
        "\n",
        "    Methods:\n",
        "    - forward(inputs): Perform a forward pass through the layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "        \"\"\"\n",
        "        Initialize a DenseLayer instance.\n",
        "\n",
        "        Parameters:\n",
        "        - n_inputs (int): Number of input features.\n",
        "        - n_neurons (int): Number of neurons in the layer.\n",
        "        \"\"\"\n",
        "        # Initialize weights and biases\n",
        "        self.weights = 0.01 * torch.rand(n_inputs, n_neurons)\n",
        "        self.biases = torch.zeros((1, n_neurons))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the layer.\n",
        "\n",
        "        Parameters:\n",
        "        - inputs (torch.Tensor): Input tensor for the forward pass.\n",
        "\n",
        "        Returns:\n",
        "        - output (torch.Tensor): Output tensor after the forward pass.\n",
        "        \"\"\"\n",
        "        # Calculate output values from inputs, weights, and biases\n",
        "        self.output = torch.matmul(inputs, self.weights) + self.biases\n",
        "        return self.output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ajjr0QcUP8tA"
      },
      "source": [
        "## Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CfScOYgaorz"
      },
      "source": [
        "### ReLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6owCQofBP_iL"
      },
      "outputs": [],
      "source": [
        "class Activation_ReLU:\n",
        "    \"\"\"\n",
        "    Rectified Linear Unit (ReLU) activation function.\n",
        "\n",
        "    The ReLU activation function is commonly used in neural networks to introduce\n",
        "    non-linearity to the model.\n",
        "\n",
        "    Methods:\n",
        "    - forward(inputs): Perform a forward pass through the ReLU activation.\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the ReLU activation.\n",
        "\n",
        "        Parameters:\n",
        "        - inputs (torch.Tensor): Input tensor for the forward pass.\n",
        "\n",
        "        Returns:\n",
        "        - output (torch.Tensor): Output tensor after the ReLU activation.\n",
        "        \"\"\"\n",
        "        self.output = torch.max(torch.tensor(0), inputs)\n",
        "        return self.output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E-apHeQahmI"
      },
      "source": [
        "### Sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nlf-0k4OaffJ"
      },
      "outputs": [],
      "source": [
        "class Activation_Sigmoid:\n",
        "    \"\"\"\n",
        "    Sigmoid activation function.\n",
        "\n",
        "    The sigmoid activation function is commonly used in neural networks to squash\n",
        "    the output of a neuron between 0 and 1.\n",
        "\n",
        "    Methods:\n",
        "    - forward(inputs): Perform a forward pass through the sigmoid activation.\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the sigmoid activation.\n",
        "\n",
        "        Parameters:\n",
        "        - inputs (torch.Tensor): Input tensor for the forward pass.\n",
        "\n",
        "        Returns:\n",
        "        - output (torch.Tensor): Output tensor after the sigmoid activation.\n",
        "        \"\"\"\n",
        "        self.output = 1 / (1 + torch.exp(inputs * -1))\n",
        "        return self.output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQrWxiXBax30"
      },
      "source": [
        "### Softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7usBC_5fprz"
      },
      "source": [
        "## Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-xWPtRUefp3j"
      },
      "outputs": [],
      "source": [
        "class Loss_MeanSquareError:\n",
        "    \"\"\"\n",
        "    Mean Square Error (MSE) loss function.\n",
        "\n",
        "    The Mean Square Error loss is commonly used in regression problems to measure\n",
        "    the average squared difference between predicted and true values.\n",
        "\n",
        "    Methods:\n",
        "    - forward(y_pred, y_true): Calculate the Mean Square Error.\n",
        "\n",
        "    ```\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Calculate the Mean Square Error (MSE).\n",
        "\n",
        "        Parameters:\n",
        "        - y_pred (torch.Tensor): Predicted values.\n",
        "        - y_true (torch.Tensor): True values.\n",
        "\n",
        "        Returns:\n",
        "        - mse_loss (torch.Tensor): Mean Square Error loss.\n",
        "        \"\"\"\n",
        "        mse_loss = torch.mean((y_pred - y_true)**2)\n",
        "        return mse_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93Dox6rDcVvN"
      },
      "source": [
        "## Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "p5KqLXjIcYQK"
      },
      "outputs": [],
      "source": [
        "class Accuracy:\n",
        "    \"\"\"\n",
        "    Accuracy metric for classification problems.\n",
        "\n",
        "    The Accuracy metric measures the proportion of correctly classified samples.\n",
        "\n",
        "    Methods:\n",
        "    - calculate(y_pred, y_true): Calculate the accuracy.\n",
        "    \"\"\"\n",
        "\n",
        "    def calculate(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Calculate the accuracy.\n",
        "\n",
        "        Parameters:\n",
        "        - y_pred (torch.Tensor): Predicted values.\n",
        "        - y_true (torch.Tensor): True values.\n",
        "\n",
        "        Returns:\n",
        "        - accuracy (torch.Tensor): Accuracy value.\n",
        "        \"\"\"\n",
        "        predictions = torch.argmax(y_pred, axis=1)\n",
        "\n",
        "        if len(y_true.shape) == 2:\n",
        "            y_true = torch.argmax(y_true, axis=1)\n",
        "\n",
        "        accuracy = torch.mean((predictions == y_true).float())\n",
        "        return accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMkTvny3m2nL"
      },
      "source": [
        "## Creating model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5bjfPNPUhzW"
      },
      "source": [
        "### BackPropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4D0Dsk5EXiu"
      },
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7FZ7Pk0gIK8v"
      },
      "outputs": [],
      "source": [
        "X = torch.tensor([0.4, 0.9])\n",
        "y = torch.tensor([0.15, 0.85])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Neural Network Layers Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Cb5W0woZRU9_"
      },
      "outputs": [],
      "source": [
        "hidden_layer_1 = DenseLayer(2, 2)\n",
        "activation1 = Activation_ReLU()\n",
        "output_layer = DenseLayer(2, 2)\n",
        "activation2 = Activation_Sigmoid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Neural Network Forward Pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAoeJRRJRArn",
        "outputId": "5457d5a0-13f2-49fa-d291-cdf55f9c8954"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.5000, 0.5000]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hidden_layer_1.forward(X)\n",
        "activation1.forward(hidden_layer_1.output)\n",
        "output_layer.forward(activation1.output)\n",
        "activation2.forward(output_layer.output)\n",
        "activation2.output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Backpropagation for a Two-Neuron Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LaQ14Al9FVU3"
      },
      "outputs": [],
      "source": [
        "def back_prop_two_neuron(fp):\n",
        "  \"\"\"\n",
        "  Perform backpropagation to update weights and biases for a two-neuron neural network.\n",
        "\n",
        "  Parameters:\n",
        "  - fp (torch.Tensor): Forward pass result, representing the output of the network.\n",
        "\n",
        "  Returns:\n",
        "  None\n",
        "  \"\"\"\n",
        "   \n",
        "  lr = torch.tensor(0.01)\n",
        "  back1 = (fp[0][0]-y[0])*(1-fp[0][0])*fp[0][0]\n",
        "  back2 = (fp[0][1]-y[1])*(1-fp[0][1])*fp[0][1]\n",
        "  output_layer.weights[0][0] -= lr * back1*activation1.output[0][0]\n",
        "  output_layer.weights[0][1] -= lr * back1*activation1.output[0][1]\n",
        "  output_layer.weights[1][0] -= lr * back2*activation1.output[0][0]\n",
        "  output_layer.weights[1][1] -= lr * back2*activation1.output[0][1]\n",
        "  output_layer.biases[0][0] -= lr * back1\n",
        "  output_layer.biases[0][1] -= lr * back2\n",
        "\n",
        "  hidden_layer_1.weights[0][0] -= lr * (back1 * output_layer.weights[0][0] * X[0] + back2 * output_layer.weights[0][1] * X[0] ) if hidden_layer_1.output[0][0] > 0 else 0\n",
        "  hidden_layer_1.weights[0][1] -= lr * (back1 * output_layer.weights[0][0] * X[1] + back2 * output_layer.weights[0][1] * X[1] ) if hidden_layer_1.output[0][0] > 0 else 0\n",
        "  hidden_layer_1.weights[1][0] -= lr * (back1 * output_layer.weights[1][0] * X[0] + back2 * output_layer.weights[1][1] * X[0] ) if hidden_layer_1.output[0][1] > 0 else 0\n",
        "  hidden_layer_1.weights[1][1] -= lr * (back1 * output_layer.weights[1][0] * X[1] + back2 * output_layer.weights[1][1] * X[1] ) if hidden_layer_1.output[0][1] > 0 else 0\n",
        "  hidden_layer_1.biases[0][0] -= lr * (back1 * output_layer.weights[0][0] + back2 * output_layer.weights[0][1] ) if hidden_layer_1.output[0][0] > 0 else 0\n",
        "  hidden_layer_1.biases[0][1] -= lr * (back1 * output_layer.weights[1][0] + back2 * output_layer.weights[1][1] ) if hidden_layer_1.output[0][1] > 0 else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJePM510S1HH",
        "outputId": "84cf763b-2a02-43d0-e97f-fdb14667eaad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output after Backpropagation:\n",
            "\n",
            "Hidden Layer 1:\n",
            "  - Weights:\n",
            " tensor([[0.0052, 0.0065],\n",
            "        [0.0018, 0.0048]])\n",
            "  - Biases:\n",
            " tensor([[-6.9484e-06,  4.8669e-06]])\n",
            "\n",
            "Output Layer:\n",
            "  - Weights:\n",
            " tensor([[0.0084, 0.0044],\n",
            "        [0.0072, 0.0100]])\n",
            "  - Biases:\n",
            " tensor([[-0.0018,  0.0017]])\n"
          ]
        }
      ],
      "source": [
        "print(\"Output after Backpropagation:\")\n",
        "back_prop_two_neuron(activation2.output)\n",
        "\n",
        "print(\"\\nHidden Layer 1:\")\n",
        "print(\"  - Weights:\\n\", hidden_layer_1.weights)\n",
        "print(\"  - Biases:\\n\", hidden_layer_1.biases)\n",
        "\n",
        "print(\"\\nOutput Layer:\")\n",
        "print(\"  - Weights:\\n\", output_layer.weights)\n",
        "print(\"  - Biases:\\n\", output_layer.biases)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
